{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioClassification2D_Gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ARBasharat/AudioClassification/blob/master/AudioClassification2D_Gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yzxm-fj2nqy"
      },
      "source": [
        "Contact: abbash@iu.edu <br/>\n",
        "Audio Spectrogram Classification using General Adversarial Network and a 2D-CNN model <br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enf2QOYNUsRX"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAve6DCL4JH4"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "from scipy import signal"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QFJAOZNFfiX"
      },
      "source": [
        "# Read Data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYA5MGNeFdH3",
        "outputId": "51f6ef02-671f-4ac7-887c-9d0fa17fbe10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "train = np.load(\"drive/My Drive/AudioClassification/audio_train.npy\").astype('float32')\n",
        "test = np.load(\"drive/My Drive/AudioClassification/audio_test.npy\").astype('float32')\n",
        "train_labels_df = pd.read_csv(\"drive/My Drive/AudioClassification/labels_train.csv\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUv0VHZH54Fj"
      },
      "source": [
        "# Process the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11jFsu6TxU8b",
        "outputId": "b7f30afb-bf2c-4b29-dbba-f71d31e47b2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_labels = train_labels_df.to_numpy()[:,1]\n",
        "labels_categorical = keras.utils.to_categorical(train_labels)\n",
        "print(\"Training Labels:\", labels_categorical.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Labels: (942, 10)\n",
            "Training Data: (942, 30000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOP7SkntypV_"
      },
      "source": [
        "def get_spectrogram(in_data):\n",
        "  spectrogram = []\n",
        "  for elem in in_data:\n",
        "    converted = librosa.stft(elem)\n",
        "    spectrum, _ = librosa.magphase(converted)\n",
        "    spectrum = np.abs(spectrum).astype(np.float32)\n",
        "    norm = spectrum.max()\n",
        "    spectrum /= norm\n",
        "    result = np.zeros((1028, 76))\n",
        "    result[:spectrum.shape[0],:spectrum.shape[1]] = spectrum\n",
        "    result = result.reshape((result.shape[0], result.shape[1], 1))\n",
        "    spectrogram.append(result)\n",
        "  spectrogram = np.array(spectrogram)\n",
        "  return spectrogram"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkyiQ0KPt3tN",
        "outputId": "aba79ed6-3353-4bc4-e3e8-4f9aa168c3d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X_train = get_spectrogram(train)\n",
        "X_test = get_spectrogram(test)\n",
        "\n",
        "print(\"Train Shape: \", X_train.shape)\n",
        "print(\"Test Shape: \", X_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Shape:  (942, 1028, 76, 1)\n",
            "Test Shape:  (558, 1028, 76, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8ot9i2BWRiG"
      },
      "source": [
        "# Model Definations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfuL-fXPLBU5",
        "outputId": "1cb547b4-e72c-493a-f1c4-b2774e220f99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(257*19*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((257, 19, 256)))\n",
        "    assert model.output_shape == (None, 257, 19, 256)\n",
        "    # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1),\n",
        "                                     padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 257, 19, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), \n",
        "                                     padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 514, 38, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), \n",
        "                                     padding='same', use_bias=False, \n",
        "                                     activation='tanh'))\n",
        "    assert model.output_shape == (None, 1028, 76, 1)\n",
        "    \n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "generator = make_generator_model()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 1250048)           125004800 \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1250048)           5000192   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 1250048)           0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 257, 19, 256)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 257, 19, 128)      819200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 257, 19, 128)      512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 257, 19, 128)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 514, 38, 64)       204800    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 514, 38, 64)       256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 514, 38, 64)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 1028, 76, 1)       1600      \n",
            "=================================================================\n",
            "Total params: 131,031,360\n",
            "Trainable params: 128,530,880\n",
            "Non-trainable params: 2,500,480\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qsWjRc4LHY3",
        "outputId": "5a6d65a6-eccd-4f98-f487-e7f2bc8c97f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[1028, 76, 1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "discriminator = make_discriminator_model()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 514, 38, 64)       1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 514, 38, 64)       0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 514, 38, 64)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 257, 19, 128)      204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 257, 19, 128)      0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 257, 19, 128)      0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 625024)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 625025    \n",
            "=================================================================\n",
            "Total params: 831,617\n",
            "Trainable params: 831,617\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiDHqVbKLoAh"
      },
      "source": [
        "# Method to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMNOMB7_LqqN"
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn_7H9ClLtiu"
      },
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeMMzwQML2Nw"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZxb0MsfL5ti"
      },
      "source": [
        "checkpoint_dir = '/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHWBO6CoME90"
      },
      "source": [
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9Y29I5IUkqN"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfjdQ34wMIBK"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "  \n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISHbVDqzMQir",
        "outputId": "f93368c3-cd52-45d7-ad13-7bb0681225b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for epoch 1 is 14.135028839111328 sec\n",
            "Time for epoch 2 is 12.717559337615967 sec\n",
            "Time for epoch 3 is 12.878023862838745 sec\n",
            "Time for epoch 4 is 12.862932443618774 sec\n",
            "Time for epoch 5 is 12.945351600646973 sec\n",
            "Time for epoch 6 is 13.007014274597168 sec\n",
            "Time for epoch 7 is 13.050602436065674 sec\n",
            "Time for epoch 8 is 13.093243598937988 sec\n",
            "Time for epoch 9 is 13.141200542449951 sec\n",
            "Time for epoch 10 is 13.177473783493042 sec\n",
            "Time for epoch 11 is 13.190034627914429 sec\n",
            "Time for epoch 12 is 13.232048749923706 sec\n",
            "Time for epoch 13 is 13.260963201522827 sec\n",
            "Time for epoch 14 is 13.272834062576294 sec\n",
            "Time for epoch 15 is 21.07028079032898 sec\n",
            "Time for epoch 16 is 13.241860151290894 sec\n",
            "Time for epoch 17 is 13.308809995651245 sec\n",
            "Time for epoch 18 is 13.313120603561401 sec\n",
            "Time for epoch 19 is 13.316790103912354 sec\n",
            "Time for epoch 20 is 13.32909345626831 sec\n",
            "Time for epoch 21 is 13.380089044570923 sec\n",
            "Time for epoch 22 is 13.39589524269104 sec\n",
            "Time for epoch 23 is 13.435005903244019 sec\n",
            "Time for epoch 24 is 13.45565390586853 sec\n",
            "Time for epoch 25 is 13.464833974838257 sec\n",
            "Time for epoch 26 is 13.468721151351929 sec\n",
            "Time for epoch 27 is 13.496897459030151 sec\n",
            "Time for epoch 28 is 13.506188869476318 sec\n",
            "Time for epoch 29 is 13.516383409500122 sec\n",
            "Time for epoch 30 is 20.656447172164917 sec\n",
            "Time for epoch 31 is 13.673378229141235 sec\n",
            "Time for epoch 32 is 13.381051301956177 sec\n",
            "Time for epoch 33 is 13.476016521453857 sec\n",
            "Time for epoch 34 is 13.501380443572998 sec\n",
            "Time for epoch 35 is 13.483006238937378 sec\n",
            "Time for epoch 36 is 13.530003070831299 sec\n",
            "Time for epoch 37 is 13.640605449676514 sec\n",
            "Time for epoch 38 is 13.467023849487305 sec\n",
            "Time for epoch 39 is 13.56348991394043 sec\n",
            "Time for epoch 40 is 13.534595489501953 sec\n",
            "Time for epoch 41 is 13.557274103164673 sec\n",
            "Time for epoch 42 is 13.55759048461914 sec\n",
            "Time for epoch 43 is 13.5654776096344 sec\n",
            "Time for epoch 44 is 13.585888862609863 sec\n",
            "Time for epoch 45 is 23.199488162994385 sec\n",
            "Time for epoch 46 is 13.403478145599365 sec\n",
            "Time for epoch 47 is 13.575988054275513 sec\n",
            "Time for epoch 48 is 13.412845373153687 sec\n",
            "Time for epoch 49 is 13.598442554473877 sec\n",
            "Time for epoch 50 is 13.423110485076904 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG7YY5_cQ4C-",
        "outputId": "b0cc0d1e-eae5-45b2-deef-8309fd4d990f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe8bc06c208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "882xlh2XM5uD"
      },
      "source": [
        "predictions = []\n",
        "for i in range(0, X_test.shape[0]):\n",
        "  y = X_test[i].reshape(1, X_test[0].shape[0], X_test[0].shape[1], X_test[0].shape[2])\n",
        "  prediction = discriminator(y, training=False)\n",
        "  predictions.append(float(prediction))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1jZJEwQXXoa"
      },
      "source": [
        "# Predict labels using 2D-CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c86xVEKOXZUY"
      },
      "source": [
        "def get_spectrogram_XX(in_data):\n",
        "  spectrogram = []\n",
        "  for elem in in_data:\n",
        "    converted = librosa.stft(elem)\n",
        "    spectrum, _ = librosa.magphase(converted)\n",
        "    spectrum = np.abs(spectrum).astype(np.float32)\n",
        "    norm = spectrum.max()\n",
        "    spectrum /= norm\n",
        "    result = np.zeros((spectrum.shape[0], 74))\n",
        "    result[:spectrum.shape[0],:spectrum.shape[1]] = spectrum\n",
        "    result = result.reshape((result.shape[0], result.shape[1], 1))\n",
        "    spectrogram.append(result)\n",
        "  spectrogram = np.array(spectrogram)\n",
        "  return spectrogram\n",
        "\n",
        "X_test_CNN = get_spectrogram_XX(test)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8srFNI0XXI8"
      },
      "source": [
        "model_1 = tf.keras.models.load_model(\"drive/My Drive/AudioClassification/model_2\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnTRev8UYbQp"
      },
      "source": [
        "predictions_probabilities = model_1.predict(X_test_CNN)\n",
        "prediction_classes = model_1.predict_classes(X_test_CNN)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6nftnbxYfdw"
      },
      "source": [
        "# Assign Labels to the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxAMTWRvXhF6"
      },
      "source": [
        "## Assign Label predicted by CNN if image is real, otherwise assign label 2\n",
        "test_labels = []\n",
        "for i in range(0, X_test.shape[0]):\n",
        "  if predictions[i] <= 0:\n",
        "    label = 2 \n",
        "  else:\n",
        "    label = prediction_classes[i]\n",
        "  test_labels.append(label)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Y3bQ_rdIBO",
        "outputId": "4cf67c48-951c-4233-e10d-99939e6942c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import collections\n",
        "counter=collections.Counter(test_labels)\n",
        "print(counter)\n",
        "len(counter)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({3: 184, 7: 55, 1: 52, 4: 51, 8: 49, 5: 43, 9: 43, 6: 39, 0: 38, 2: 4})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYEOd9WodZM4"
      },
      "source": [
        "pd.DataFrame(test_labels).to_csv(\"submission_GAN.csv\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXtDXH1bZDe5",
        "outputId": "eda5d556-1489-442a-9946-8916ec9012fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Test_Data_Index\", \",\", \"Assigned Label\")\n",
        "for i in range(0, len(test_labels)):\n",
        "  print(i, \",\", test_labels[i])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test_Data_Index , Assigned Label\n",
            "0 , 5\n",
            "1 , 9\n",
            "2 , 4\n",
            "3 , 1\n",
            "4 , 4\n",
            "5 , 5\n",
            "6 , 9\n",
            "7 , 3\n",
            "8 , 3\n",
            "9 , 7\n",
            "10 , 7\n",
            "11 , 0\n",
            "12 , 1\n",
            "13 , 8\n",
            "14 , 0\n",
            "15 , 7\n",
            "16 , 7\n",
            "17 , 3\n",
            "18 , 5\n",
            "19 , 3\n",
            "20 , 3\n",
            "21 , 3\n",
            "22 , 9\n",
            "23 , 3\n",
            "24 , 3\n",
            "25 , 5\n",
            "26 , 7\n",
            "27 , 7\n",
            "28 , 0\n",
            "29 , 4\n",
            "30 , 4\n",
            "31 , 1\n",
            "32 , 3\n",
            "33 , 1\n",
            "34 , 6\n",
            "35 , 3\n",
            "36 , 1\n",
            "37 , 3\n",
            "38 , 9\n",
            "39 , 9\n",
            "40 , 0\n",
            "41 , 1\n",
            "42 , 5\n",
            "43 , 9\n",
            "44 , 8\n",
            "45 , 7\n",
            "46 , 3\n",
            "47 , 3\n",
            "48 , 3\n",
            "49 , 6\n",
            "50 , 9\n",
            "51 , 6\n",
            "52 , 0\n",
            "53 , 0\n",
            "54 , 8\n",
            "55 , 3\n",
            "56 , 5\n",
            "57 , 3\n",
            "58 , 3\n",
            "59 , 3\n",
            "60 , 3\n",
            "61 , 7\n",
            "62 , 8\n",
            "63 , 7\n",
            "64 , 0\n",
            "65 , 0\n",
            "66 , 4\n",
            "67 , 6\n",
            "68 , 3\n",
            "69 , 3\n",
            "70 , 3\n",
            "71 , 4\n",
            "72 , 3\n",
            "73 , 5\n",
            "74 , 1\n",
            "75 , 4\n",
            "76 , 9\n",
            "77 , 7\n",
            "78 , 4\n",
            "79 , 5\n",
            "80 , 0\n",
            "81 , 1\n",
            "82 , 6\n",
            "83 , 9\n",
            "84 , 7\n",
            "85 , 3\n",
            "86 , 3\n",
            "87 , 3\n",
            "88 , 1\n",
            "89 , 6\n",
            "90 , 0\n",
            "91 , 8\n",
            "92 , 0\n",
            "93 , 8\n",
            "94 , 7\n",
            "95 , 7\n",
            "96 , 3\n",
            "97 , 3\n",
            "98 , 3\n",
            "99 , 3\n",
            "100 , 3\n",
            "101 , 3\n",
            "102 , 7\n",
            "103 , 3\n",
            "104 , 7\n",
            "105 , 3\n",
            "106 , 3\n",
            "107 , 4\n",
            "108 , 1\n",
            "109 , 3\n",
            "110 , 3\n",
            "111 , 3\n",
            "112 , 7\n",
            "113 , 7\n",
            "114 , 1\n",
            "115 , 9\n",
            "116 , 9\n",
            "117 , 4\n",
            "118 , 5\n",
            "119 , 1\n",
            "120 , 5\n",
            "121 , 9\n",
            "122 , 3\n",
            "123 , 4\n",
            "124 , 3\n",
            "125 , 3\n",
            "126 , 6\n",
            "127 , 3\n",
            "128 , 0\n",
            "129 , 3\n",
            "130 , 8\n",
            "131 , 7\n",
            "132 , 3\n",
            "133 , 3\n",
            "134 , 3\n",
            "135 , 3\n",
            "136 , 7\n",
            "137 , 3\n",
            "138 , 5\n",
            "139 , 4\n",
            "140 , 4\n",
            "141 , 1\n",
            "142 , 4\n",
            "143 , 8\n",
            "144 , 7\n",
            "145 , 3\n",
            "146 , 3\n",
            "147 , 3\n",
            "148 , 1\n",
            "149 , 5\n",
            "150 , 0\n",
            "151 , 9\n",
            "152 , 5\n",
            "153 , 3\n",
            "154 , 3\n",
            "155 , 9\n",
            "156 , 3\n",
            "157 , 9\n",
            "158 , 4\n",
            "159 , 0\n",
            "160 , 3\n",
            "161 , 3\n",
            "162 , 3\n",
            "163 , 3\n",
            "164 , 3\n",
            "165 , 7\n",
            "166 , 7\n",
            "167 , 3\n",
            "168 , 5\n",
            "169 , 0\n",
            "170 , 4\n",
            "171 , 0\n",
            "172 , 8\n",
            "173 , 8\n",
            "174 , 6\n",
            "175 , 3\n",
            "176 , 3\n",
            "177 , 3\n",
            "178 , 6\n",
            "179 , 5\n",
            "180 , 5\n",
            "181 , 1\n",
            "182 , 0\n",
            "183 , 7\n",
            "184 , 8\n",
            "185 , 1\n",
            "186 , 0\n",
            "187 , 1\n",
            "188 , 3\n",
            "189 , 3\n",
            "190 , 9\n",
            "191 , 6\n",
            "192 , 1\n",
            "193 , 4\n",
            "194 , 8\n",
            "195 , 8\n",
            "196 , 4\n",
            "197 , 5\n",
            "198 , 3\n",
            "199 , 3\n",
            "200 , 7\n",
            "201 , 2\n",
            "202 , 4\n",
            "203 , 1\n",
            "204 , 6\n",
            "205 , 6\n",
            "206 , 9\n",
            "207 , 9\n",
            "208 , 6\n",
            "209 , 3\n",
            "210 , 3\n",
            "211 , 9\n",
            "212 , 5\n",
            "213 , 9\n",
            "214 , 1\n",
            "215 , 7\n",
            "216 , 0\n",
            "217 , 5\n",
            "218 , 1\n",
            "219 , 4\n",
            "220 , 9\n",
            "221 , 3\n",
            "222 , 3\n",
            "223 , 3\n",
            "224 , 6\n",
            "225 , 4\n",
            "226 , 8\n",
            "227 , 5\n",
            "228 , 8\n",
            "229 , 7\n",
            "230 , 4\n",
            "231 , 3\n",
            "232 , 3\n",
            "233 , 7\n",
            "234 , 5\n",
            "235 , 8\n",
            "236 , 6\n",
            "237 , 8\n",
            "238 , 3\n",
            "239 , 3\n",
            "240 , 6\n",
            "241 , 9\n",
            "242 , 6\n",
            "243 , 9\n",
            "244 , 4\n",
            "245 , 5\n",
            "246 , 0\n",
            "247 , 7\n",
            "248 , 9\n",
            "249 , 3\n",
            "250 , 9\n",
            "251 , 3\n",
            "252 , 3\n",
            "253 , 8\n",
            "254 , 0\n",
            "255 , 5\n",
            "256 , 8\n",
            "257 , 7\n",
            "258 , 3\n",
            "259 , 3\n",
            "260 , 3\n",
            "261 , 3\n",
            "262 , 7\n",
            "263 , 3\n",
            "264 , 3\n",
            "265 , 3\n",
            "266 , 5\n",
            "267 , 4\n",
            "268 , 5\n",
            "269 , 8\n",
            "270 , 6\n",
            "271 , 9\n",
            "272 , 3\n",
            "273 , 3\n",
            "274 , 3\n",
            "275 , 6\n",
            "276 , 9\n",
            "277 , 0\n",
            "278 , 9\n",
            "279 , 1\n",
            "280 , 1\n",
            "281 , 5\n",
            "282 , 1\n",
            "283 , 7\n",
            "284 , 3\n",
            "285 , 9\n",
            "286 , 3\n",
            "287 , 8\n",
            "288 , 7\n",
            "289 , 3\n",
            "290 , 8\n",
            "291 , 0\n",
            "292 , 3\n",
            "293 , 3\n",
            "294 , 0\n",
            "295 , 3\n",
            "296 , 5\n",
            "297 , 3\n",
            "298 , 3\n",
            "299 , 3\n",
            "300 , 8\n",
            "301 , 8\n",
            "302 , 0\n",
            "303 , 0\n",
            "304 , 4\n",
            "305 , 8\n",
            "306 , 8\n",
            "307 , 6\n",
            "308 , 6\n",
            "309 , 3\n",
            "310 , 3\n",
            "311 , 1\n",
            "312 , 3\n",
            "313 , 7\n",
            "314 , 2\n",
            "315 , 4\n",
            "316 , 1\n",
            "317 , 3\n",
            "318 , 3\n",
            "319 , 3\n",
            "320 , 3\n",
            "321 , 8\n",
            "322 , 4\n",
            "323 , 5\n",
            "324 , 6\n",
            "325 , 7\n",
            "326 , 3\n",
            "327 , 3\n",
            "328 , 0\n",
            "329 , 3\n",
            "330 , 6\n",
            "331 , 3\n",
            "332 , 7\n",
            "333 , 4\n",
            "334 , 1\n",
            "335 , 4\n",
            "336 , 3\n",
            "337 , 9\n",
            "338 , 3\n",
            "339 , 3\n",
            "340 , 1\n",
            "341 , 6\n",
            "342 , 7\n",
            "343 , 9\n",
            "344 , 9\n",
            "345 , 3\n",
            "346 , 3\n",
            "347 , 6\n",
            "348 , 8\n",
            "349 , 8\n",
            "350 , 8\n",
            "351 , 4\n",
            "352 , 3\n",
            "353 , 8\n",
            "354 , 6\n",
            "355 , 3\n",
            "356 , 3\n",
            "357 , 6\n",
            "358 , 8\n",
            "359 , 7\n",
            "360 , 3\n",
            "361 , 4\n",
            "362 , 7\n",
            "363 , 8\n",
            "364 , 3\n",
            "365 , 8\n",
            "366 , 3\n",
            "367 , 6\n",
            "368 , 3\n",
            "369 , 6\n",
            "370 , 9\n",
            "371 , 9\n",
            "372 , 9\n",
            "373 , 0\n",
            "374 , 1\n",
            "375 , 1\n",
            "376 , 7\n",
            "377 , 6\n",
            "378 , 9\n",
            "379 , 1\n",
            "380 , 1\n",
            "381 , 3\n",
            "382 , 3\n",
            "383 , 3\n",
            "384 , 8\n",
            "385 , 3\n",
            "386 , 4\n",
            "387 , 0\n",
            "388 , 0\n",
            "389 , 7\n",
            "390 , 7\n",
            "391 , 3\n",
            "392 , 3\n",
            "393 , 8\n",
            "394 , 3\n",
            "395 , 7\n",
            "396 , 7\n",
            "397 , 5\n",
            "398 , 4\n",
            "399 , 4\n",
            "400 , 5\n",
            "401 , 4\n",
            "402 , 3\n",
            "403 , 3\n",
            "404 , 4\n",
            "405 , 3\n",
            "406 , 3\n",
            "407 , 0\n",
            "408 , 1\n",
            "409 , 5\n",
            "410 , 5\n",
            "411 , 7\n",
            "412 , 1\n",
            "413 , 4\n",
            "414 , 1\n",
            "415 , 5\n",
            "416 , 3\n",
            "417 , 3\n",
            "418 , 6\n",
            "419 , 3\n",
            "420 , 6\n",
            "421 , 4\n",
            "422 , 1\n",
            "423 , 8\n",
            "424 , 7\n",
            "425 , 3\n",
            "426 , 9\n",
            "427 , 6\n",
            "428 , 8\n",
            "429 , 3\n",
            "430 , 3\n",
            "431 , 3\n",
            "432 , 3\n",
            "433 , 3\n",
            "434 , 3\n",
            "435 , 3\n",
            "436 , 7\n",
            "437 , 1\n",
            "438 , 1\n",
            "439 , 8\n",
            "440 , 0\n",
            "441 , 4\n",
            "442 , 0\n",
            "443 , 6\n",
            "444 , 3\n",
            "445 , 6\n",
            "446 , 4\n",
            "447 , 3\n",
            "448 , 3\n",
            "449 , 5\n",
            "450 , 4\n",
            "451 , 9\n",
            "452 , 1\n",
            "453 , 5\n",
            "454 , 5\n",
            "455 , 4\n",
            "456 , 3\n",
            "457 , 3\n",
            "458 , 4\n",
            "459 , 4\n",
            "460 , 0\n",
            "461 , 1\n",
            "462 , 5\n",
            "463 , 7\n",
            "464 , 7\n",
            "465 , 3\n",
            "466 , 3\n",
            "467 , 9\n",
            "468 , 3\n",
            "469 , 3\n",
            "470 , 9\n",
            "471 , 3\n",
            "472 , 8\n",
            "473 , 3\n",
            "474 , 8\n",
            "475 , 5\n",
            "476 , 1\n",
            "477 , 8\n",
            "478 , 6\n",
            "479 , 1\n",
            "480 , 4\n",
            "481 , 3\n",
            "482 , 3\n",
            "483 , 4\n",
            "484 , 5\n",
            "485 , 5\n",
            "486 , 0\n",
            "487 , 9\n",
            "488 , 1\n",
            "489 , 5\n",
            "490 , 9\n",
            "491 , 6\n",
            "492 , 0\n",
            "493 , 3\n",
            "494 , 2\n",
            "495 , 0\n",
            "496 , 1\n",
            "497 , 8\n",
            "498 , 0\n",
            "499 , 3\n",
            "500 , 8\n",
            "501 , 8\n",
            "502 , 8\n",
            "503 , 7\n",
            "504 , 3\n",
            "505 , 3\n",
            "506 , 3\n",
            "507 , 9\n",
            "508 , 3\n",
            "509 , 3\n",
            "510 , 3\n",
            "511 , 8\n",
            "512 , 3\n",
            "513 , 8\n",
            "514 , 1\n",
            "515 , 1\n",
            "516 , 3\n",
            "517 , 3\n",
            "518 , 3\n",
            "519 , 6\n",
            "520 , 1\n",
            "521 , 7\n",
            "522 , 4\n",
            "523 , 1\n",
            "524 , 3\n",
            "525 , 6\n",
            "526 , 4\n",
            "527 , 7\n",
            "528 , 3\n",
            "529 , 3\n",
            "530 , 4\n",
            "531 , 1\n",
            "532 , 1\n",
            "533 , 3\n",
            "534 , 3\n",
            "535 , 7\n",
            "536 , 3\n",
            "537 , 3\n",
            "538 , 3\n",
            "539 , 3\n",
            "540 , 3\n",
            "541 , 3\n",
            "542 , 7\n",
            "543 , 7\n",
            "544 , 3\n",
            "545 , 3\n",
            "546 , 4\n",
            "547 , 4\n",
            "548 , 8\n",
            "549 , 2\n",
            "550 , 3\n",
            "551 , 3\n",
            "552 , 3\n",
            "553 , 1\n",
            "554 , 1\n",
            "555 , 5\n",
            "556 , 7\n",
            "557 , 5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}